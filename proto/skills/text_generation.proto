syntax = "proto3";

package jailbird.skills.textgeneration;

import "common/types.proto";
import "common/health.proto";
import "google/api/annotations.proto";

option java_multiple_files = true;
option java_package = "com.jailbird.skills.textgeneration";
option java_outer_classname = "TextGenerationProtos";

// Text generation service for LLM integration
service TextGenerationService {
    // Generate text based on prompt and context
    rpc GenerateText(GenerateTextRequest) returns (GenerateTextResponse) {
        option (google.api.http) = {
            post: "/generate_text"
            body: "*"
        };
    }
    
    // Stream text generation for real-time responses
    rpc GenerateTextStream(GenerateTextRequest) returns (stream GenerateTextResponse) {
        option (google.api.http) = {
            post: "/generate_text_stream"
            body: "*"
        };
    }
    
    // Health check
    rpc Health(jailbird.common.HealthCheckRequest) returns (jailbird.common.HealthCheckResponse) {
        option (google.api.http) = {
            get: "/health"
        };
    }
    
    // Get available models - TEST ENDPOINT for proto-first workflow
    rpc GetModels(GetModelsRequest) returns (GetModelsResponse) {
        option (google.api.http) = {
            get: "/get_models"
        };
    }
}

message GenerateTextRequest {
    string prompt = 1;
    jailbird.common.Context context = 2;
    
    // Generation parameters
    GenerationConfig config = 3;
    
    // Optional system message
    string system_message = 4;
    
    // Conversation history (for context)
    repeated ChatMessage history = 5;
}

message GenerateTextResponse {
    string generated_text = 1;
    
    // Metadata about generation
    GenerationMetadata metadata = 2;
    
    // Error information if generation failed
    jailbird.common.ErrorInfo error = 3;
    
    // Streaming info
    bool is_final = 4;
    int32 token_count = 5;
}

message GenerationConfig {
    // Model parameters
    double temperature = 1;      // 0.0 to 2.0
    int32 max_tokens = 2;        // Maximum tokens to generate
    double top_p = 3;            // 0.0 to 1.0
    int32 top_k = 4;             // Top-k sampling
    repeated string stop_sequences = 5;
    
    // Persona-specific settings
    bool use_persona_prompt = 6;
    map<string, string> persona_overrides = 7;
}

message ChatMessage {
    string role = 1;             // "user", "assistant", "system"
    string content = 2;
    int64 timestamp = 3;
    map<string, string> metadata = 4;
}

message GenerationMetadata {
    string model_name = 1;
    int32 input_tokens = 2;
    int32 output_tokens = 3;
    double generation_time_ms = 4;
    double tokens_per_second = 5;
    string finish_reason = 6;    // "stop", "length", "error"
}

// TEST MESSAGES for proto-first workflow verification
message GetModelsRequest {
    // Empty for now - could add filtering parameters later
}

message GetModelsResponse {
    repeated ModelInfo models = 1;
}

message ModelInfo {
    string model_id = 1;
    string display_name = 2;
    string description = 3;
    bool is_available = 4;
    map<string, string> capabilities = 5;
}